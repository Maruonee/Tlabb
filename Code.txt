#########################################dataedit
#install 
conda create -n ConVformat python=3.8
conda activate ConVformat
pip install numpy opencv-python tqdm sklearn split-folder scikit-learn
git clone https://github.com/Taeyoung96/Yolo-to-COCO-format-converter.git

#activate
cd /home/tlab/Tlab/yolo2coco
conda activate ConVformat
cd /home/tlab/Tlabb/editdata/yolo2coco
python main.py --path /home/tlab/Tlabb/editdata/yolo2coco/tutorial/test --output test.json

#activate
cd /home/tlab/Tlab/voc2coco
python3 voc2coco.py --ann_dir /media/tlab/Data/Dataset/Mask/coco/VOC2007/Annotations --ann_ids /media/tlab/Data/Dataset/Mask/coco/VOC2007/ImageSets/Main/val.txt --labels /media/tlab/Data/Dataset/Mask/classes.txt --output /media/tlab/Data/Dataset/Mask/coco/VOC2007/output.json --ext xml

#########################################YOLOV3
#install 
conda create -n YOLOV3 python=3.8
conda activate YOLOV3
conda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=11.1 -c pytorch -c conda-forge wandb
cd /home/tlab1004/Tlabb/ObjectDetection/YOLOV3
pip install wandb -r requirements.txt

#activate
conda activate YOLOV3
cd /home/tlab1004/Tlabb/ObjectDetection/YOLOV3

#Pre train(optimizer hyperparameter)
python train.py --cfg /home/tlab1004/dataset/opt/yolov3.yaml --data /home/tlab1004/dataset/opt/coco.yaml --weights yolov3.pt --batch 32 --epochs 10 --cache --evolve --img 512

#Train 
python train.py --cfg /home/tlab1004/dataset/opt/yolov3.yaml --data /home/tlab1004/dataset/opt/coco.yaml --weights yolov3.pt --batch 32 --epochs 500 --img 512 --hyp /home/tlab1004/dataset/opt/V3/hyp_evolve.yaml

python train.py --cfg /home/tlab1004/dataset/opt/yolov3.yaml --data /home/tlab1004/dataset/opt/coco.yaml --weights '' --batch 32 --epochs 500 --img 512

#detect
python detect.py --weights '/home/tlab1004/dataset/Res/results/CT_Res_YOLOV3_transfer/weights/best.pt' --source '/home/tlab1004/dataset/Res/images/*.tif' --save-conf --save-txt --conf-thres 0.80 --name 'con0.8' --save-crop --line-thickness 1olor_p6.pt --device 0 --epochs 500 --cache

#detect
python detect.py --source '/home/tlab1004/dataset/Con/images/*.tif' --cfg /home/tlab1004/dataset/Con/opt/yolor_p6.cfg --weights /home/tlab1004/dataset/Con/results/CT_Con_YOLOR_transfer/weights/best.pt --save-txt --conf-thres 0.7

#test
python test.py --data /home/tlab/dataset/o
 
#eval
python val.py --data /home/tlab1004/dataset/opt/coco128.yaml --weights /home/tlab1004/dataset/data/two/YOLOV3.pt --img 512 --verbose --save-txt --save-hybrid --augment --save-json --exist-ok


conda env remove -n seg


##########################################YOLOv5
#install 
conda create -n YOLOV5 python=3.8
conda activate YOLOV5
cd /home/tlab4090/Tlabb/ObjectDetection/YOLOV5
pip install wandb -r requirements.txt
conda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=11.1 -c pytorch -c conda-forge

#activate
conda activate YOLOV5
cd /home/tlab1004/Tlabb/ObjectDetection/YOLOV5

#Pre train(optimizer hyperparameter)
python train.py --cfg /home/tlab1004/dataset/opt/yolov5n.yaml --data /home/tlab1004/dataset/opt/coco128.yaml --batch 32 --epochs 10 --cache --evolve --img 512

#Train 
python train.py --cfg /home/tlab4090/datasets/opt/yolov5n.yaml --data /home/tlab4090/datasets/opt/coco.yaml --weights yolov5m.pt --batch 32 --epochs 500 --img 512 --name c_arm

python train.py --cfg /home/tlab1004/dataset/opt/yolov5m.yaml --data /home/tlab1004/dataset/opt/coco.yaml --weights '' --batch 32 --epochs 500 --img 512

#detect
python detect.py --weights /home/tlab1004/dataset/Con/results/CT_Con_YOLOV5_transfer/weights/best.pt --source '/home/tlab1004/dataset/Con/images/*.tif' --save-conf --save-txt --conf-thres 0.7 --save-crop --line-thickness 1 --name 'con0.7' 

#eval
python val.py --data /home/tlab/dataset/opt/coco128.yaml --weights /home/tlab1004/dataset/data/two/YOLOV5.pt --img 512

python val.py --data /home/tlab1004/dataset/opt/coco128.yaml --weights /home/tlab1004/dataset/data/two/YOLOV3.pt --img 512
########################################YOLOR
#install	
conda create -n YOLORR python=3.8
conda activate YOLORR
cd /home/tlab1004/Tlabb/ObjectDetection/YOLOR
pip install seaborn thop cython opencv-contrib-python
conda install pytorch==1.11.0 torchvision==0.12.0 torchaudio==0.11.0 cudatoolkit=11.3 -c pytorch
pip install -r requirements.txt

#activate 
conda activate YOLORR
cd /home/tlab1004/Tlabb/ObjectDetection/YOLOR

#Pre train(optimizer hyperparameter)
python train.py --batch-size 32 --img 512 512 --data /home/tlab1004/dataset/opt/coco.yaml --cfg /home/tlab1004/dataset/opt/yolor_p6.cfg --weights yolor_p6.pt --device 0 --epochs 10 --evolve --cache

#train
python train.py --batch-size 32 --img 512 512 --data /home/tlab1004/dataset/opt/coco.yaml --cfg /home/tlab1004/dataset/opt/yolor_p6.cfg --weights ypt/coco.yaml --img-size 512 --batch-size 32 --device 0 --cfg /home/tlab/dataset/opt/yolor_p6.cfg --weights /home/tlab/results/r/weights/best.pt --name yolor_1_val --verbose --save-txt --save-conf

########################################YOLOV7
#install
conda create -n YOLOV77 python=3.8
conda activate YOLOV77
cd /home/tlab1004/Tlabb/ObjectDetection/YOLOV7
conda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=11.1 -c pytorch -c conda-forge
pip install -r requirements.txt
pip install wandb

#activate 
conda activate YOLOV77
cd /home/tlab1004/Tlabb/ObjectDetection/YOLOV7

#train
python train.py --cfg /home/tlab4090/datasets/opt/yolov7.yaml --data /home/tlab4090/datasets/opt/coco.yaml --weights yolov7.pt --batch 32 --epochs 500 --img 512 --device 0 --workers 32 --name C_arm_sho

 --evolve
 
python train.py --cfg /home/tlab1004/dataset/opt/yolov7.yaml --data /home/tlab1004/dataset/opt/coco.yaml --weights yolov7.pt --batch 32 --epochs 500 --img 512 --device 0 --workers 16 --name ctspitial_1

#Test
python test.py --data /home/tlab/dataset/opt/coco.yaml --img 512 --batch 32 --weights /home/tlab/results/v7/weights/best.pt  --workers16

#detect
python detect.py --weights '/home/tlab1004/dataset/Con/results/CT_Con_YOLOV7_nontransfer/weights/best.pt' --source '/home/tlab1004/dataset/Con/images/*.tif' --device 0 --save-conf --save-txt --conf-thres 0.6 --name 'con0.6' 

#Pose Training
cd /home/tlab1004/Tlabb/ObjectDetection/YOLOV7_P
python train.py --data data/coco_kpts.yaml --cfg cfg/yolov7-w6-pose.yaml --weights weights/yolov7-w6-person.pt --batch-size 128 --img 960 --kpt-label --sync-bn --device 0 --name yolov7-w6-pose --hyp data/hyp.pose.yaml

#Pose test
python test.py --data data/coco_kpts.yaml --img 960 --conf 0.001 --iou 0.65 --weights yolov7-w6-pose.pt --kpt-label

#Pose detect
python detect.py --weights yolov7-w6-pose.pt --source /home/tlab1004/Tlabb/ObjectDetection/YOLOV7_P/data/IMG_0413.MP4 --kpt-label --line-thickness 8 --hide-labels --hide-conf --nobbox

########################################YOLOV8
conda create -n YOLOV8 python=3.8
conda activate YOLOV8
cd /home/tlab4090/Tlabb/ObjectDetection/myyolo
cd /home/tlab1004/Tlabb/ObjectDetection/myyolo
pip install ultralytics

conda activate YOLOV8

#사용법
yolo TASK MODE ARGS
 TASK : [detect, segment, classify, pose].
 MODE : [train, val, predict, export, track, benchmark]
 ARGS : options

#train
yolo train model=None data='/home/tlab1004/dataset/opt/coco.yaml' batch=32 epochs=500 imgsz=512 save=True device=0 workers=16 name=gauze_normal

yolo train data='/home/tlab4090/datasets/opt/coco.yaml' model=yolov8n.yaml epochs=300 imgsz=512 batch=32 device=0 workers=32 name=c_arm_needle save=True

#detect
yolo detect predict model='/home/tlab4090/Tlabb/ObjectDetection/myyolo/runs/detect/c_arm_needle_6/weights/best.pt' source='/home/tlab4090/datasets/opt/test.txt' conf=0.7 device=0 save_txt=True save_conf=True save_crop=True line_thickness=2 save=True iou=0.45 workers=16 name=test

#seg_train
yolo segment train data=/home/tlab4090/datasets/opt/coco128-seg.yaml model=yolov8n-seg.yaml epochs=500 imgsz=512 batch=32 device=0 workers=32 save=True cache=True patience=0
yolo segment train data=/home/tlab1004/datasets/opt/coco128-seg.yaml model=yolov8n-seg.yaml epochs=500 imgsz=512 batch=32 device=0 workers=16 save=True cache=True patience=0

#val
yolo segment val model=yolov8n-seg.pt  # val official model

#detect
yolo segment predict model=yolov8n-seg.pt source='https://ultralytics.com/images/bus.jpg'  # predict with official model 



# train
model	None	path to model file, i.e. yolov8n.pt, yolov8n.yaml
data	None	path to data file, i.e. coco128.yaml
epochs	100	number of epochs to train for
patience	50	epochs to wait for no observable improvement for early stopping of training
batch	16	number of images per batch (-1 for AutoBatch)
imgsz	640	size of input images as integer or w,h
save	True	save train checkpoints and predict results
save_period	-1	Save checkpoint every x epochs (disabled if < 1)
cache	False	True/ram, disk or False. Use cache for data loading
device	None	device to run on, i.e. cuda device=0 or device=0,1,2,3 or device=cpu
workers	8	number of worker threads for data loading (per RANK if DDP)
project	None	project name
name	None	experiment name
exist_ok	False	whether to overwrite existing experiment
pretrained	False	whether to use a pretrained model
optimizer	'SGD'	optimizer to use, choices=['SGD', 'Adam', 'AdamW', 'RMSProp']
verbose	False	whether to print verbose output
seed	0	random seed for reproducibility
deterministic	True	whether to enable deterministic mode
single_cls	False	train multi-class data as single-class
image_weights	False	use weighted image selection for training
rect	False	rectangular training train only)
val	True	validate/test during training

#Predict
source	'ultralytics/assets'	source directory for images or videos
conf	0.25	object confidence threshold for detection
iou	0.7	intersection over union (IoU) threshold for NMS
half	False	use half precision (FP16)
device	None	device to run on, i.e. cuda device=0/1/2/3 or device=cpu
show	False	show results if possible
save	False	save images with results
save_txt	False	save results as .txt file
save_conf	False	save results with confidence scores
save_crop	False	save cropped images with results
show_labels	True	show object labels in plots
show_conf	True	show object confidence scores in plots
max_det	300	maximum number of detections per image
vid_stride	False	video frame-rate stride
line_thickness	3	bounding box thickness (pixels)
visualize	False	visualize model features
augment	False	apply image augmentation to prediction sources
agnostic_nms	False	class-agnostic NMS
retina_masks	False	use high-resolution segmentation masks
classes	None	filter results by class, i.e. class=0, or class=[0,2,3]
boxes	True	Show boxes in segmentation predictions

#val
save_json	False	save results to JSON file
save_hybrid	False	save hybrid version of labels (labels + additional predictions)
conf	0.001	object confidence threshold for detection
iou	0.6	intersection over union (IoU) threshold for NMS
max_det	300	maximum number of detections per image
half	True	use half precision (FP16)
device	None	device to run on, i.e. cuda device=0/1/2/3 or device=cpu
dnn	False	use OpenCV DNN for ONNX inference
plots	False	show plots during training
rect	False	rectangular val with each batch collated for minimum padding
split	val	dataset split to use for validation, i.e. 'val', 'test' or 'train'

#Export
format	'torchscript'	format to export to
imgsz	640	image size as scalar or (h, w) list, i.e. (640, 480)
keras	False	use Keras for TF SavedModel export
optimize	False	TorchScript: optimize for mobile
half	False	FP16 quantization
int8	False	INT8 quantization
dynamic	False	ONNX/TF/TensorRT: dynamic axes
simplify	False	ONNX: simplify model
opset	None	ONNX: opset version (optional, defaults to latest)
workspace	4	TensorRT: workspace size (GB)
nms	False	CoreML: add NMS

#Augmentation
hsv_h	0.015	image HSV-Hue augmentation (fraction)
hsv_s	0.7	image HSV-Saturation augmentation (fraction)
hsv_v	0.4	image HSV-Value augmentation (fraction)
degrees	0.0	image rotation (+/- deg)
translate	0.1	image translation (+/- fraction)
scale	0.5	image scale (+/- gain)
shear	0.0	image shear (+/- deg)
perspective	0.0	image perspective (+/- fraction), range 0-0.001
flipud	0.0	image flip up-down (probability)
fliplr	0.5	image flip left-right (probability)
mosaic	1.0	image mosaic (probability)
mixup	0.0	image mixup (probability)
copy_paste	0.0	segment copy-paste (probability)




########################################YOLOnas
#install
conda create -n YOLOnas python=3.8
conda activate YOLOnas
cd /home/tlab1004/Tlabb/ObjectDetection/YOLOnas



########################################Alphapose
cd /home/tlab1004/Tlabb/Pose/AlphaPose
conda create -n ahpose python=3.7 -y
conda activate ahpose
conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia
export PATH=/usr/local/cuda/bin/:$PATH
export LD_LIBRARY_PATH=/usr/local/cuda/lib64/:$LD_LIBRARY_PATH
python -m pip install cython
pip install opencv-python matplotlib tqdm natsort scipy cython_bbox easydict pyyaml pycocotools

sudo apt-get install libyaml-dev
python setup.py build develop

#detect
python scripts/demo_inference.py --cfg configs/coco/resnet/256x192_res152_lr1e-3_1x-duc.yaml --checkpoint pretrained_models/fast_421_res152_256x192.pth --indir /home/tlab1004/dataset/ChestPA/raw/train/Pass/ --outdir runs/exp --save_img --detector yolo --showbox --vis_fast

########################################MMLAb

cd /home/tlab4090/Tlabb/Pose/mmpose
conda create --name seg1 python=3.8 -y
conda activate seg1
conda install pytorch==1.11.0 torchvision==0.12.0 torchaudio==0.11.0 cudatoolkit=11.3 -c pytorch
pip install -U openmim
mim install mmengine
mim install "mmcv>=2.0.0"
git clone -b main https://github.com/open-mmlab/mmsegmentation.git
cd mmsegmentation
pip install -v -e .

# demo test
wget https://download.openmmlab.com/mmsegmentation/v0.5/pspnet/pspnet_r50-d8_512x1024_40k_cityscapes/pspnet_r50-d8_512x1024_40k_cityscapes_20200605_003338-2966598c.pth

python demo/image_demo.py --img demo/demo.png --config configs/pspnet/pspnet_r50-d8_4xb2-40k_cityscapes-512x1024.py pspnet_r50-d8_512x1024_40k_cityscapes_20200605_003338-2966598c.pth --device cuda:0 --out result.jpg

========================pose=================
python demo/top_down_img_demo_with_mmdet.py \
    demo/mmdetection_cfg/faster_rcnn_r50_fpn_coco.py \
    https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth \
    configs/wholebody/2d_kpt_sview_rgb_img/topdown_heatmap/coco-wholebody/hrnet_w48_coco_wholebody_384x288_dark_plus.py \
    https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w48_coco_wholebody_384x288_dark-f5726563_20200918.pth \
    --img-root /home/tlab1004/dataset/rawclass/Fail/ \
    --out-img-root runs/exp

python demo/bottom_up_img_demo.py \
    configs/body/2d_kpt_sview_rgb_img/associative_embedding/coco/hrnet_w32_coco_512x512.py \
    https://download.openmmlab.com/mmpose/bottom_up/hrnet_w32_coco_512x512-bcb8c247_20200816.pth \
    --img-path /home/tlab1004/dataset/ChestPA/raw/train/Pass/ \
    --out-img-root runs/exp --radius 10 --thickness 10
    
configs/body/2d_kpt_sview_rgb_img/associative_embedding/aic/hrnet_w32_aic_512x512.py



########################################segmantaion openmmlab
cd /home/tlab4090/Tlabb/segman/mmlab
conda create --name segman python=3.8 -y
conda activate segman
conda install pytorch==1.11.0 torchvision==0.12.0 torchaudio==0.11.0 cudatoolkit=11.3 -c pytorch
pip install -U openmim
mim install mmengine
mim install "mmcv>=2.0.0"
git clone -b main https://github.com/open-mmlab/mmsegmentation.git
cd mmsegmentation
pip install -v -e .

#activate 
conda acitvate segman
cd /home/tlab4090/Tlabb/segman/mmlab

#test
mim download mmsegmentation --config pspnet_r50-d8_4xb2-40k_cityscapes-512x1024 --dest .

python demo/image_demo.py demo/demo.png configs/pspnet/pspnet_r50-d8_4xb2-40k_cityscapes-512x1024.py pspnet_r50-d8_512x1024_40k_cityscapes_20200605_003338-2966598c.pth --device cuda:0 --out-file result.jpg



#dataset(based on cityscapes)
python tools/dataset_converters/cityscapes.py data/cityscapes --nproc 8

#dataset_basedon_HRF
python tools/dataset_converters/hrf.py data/HRF/healthy.zip data/HRF/healthy_manualsegm.zip data/HRF/glaucoma.zip data/HRF/glaucoma_manualsegm.zip data/HRF/diabetic_retinopathy.zip data/HRF/diabetic_retinopathy_manualsegm.zip

#train(based on cityscapes)
python tools/train.py /home/tlab1004/Tlabb/segman/mmlab/configs/deeplabv3plus/deeplabv3plus_r50-d8_4xb2-80k_cityscapes-512x1024.py --amp

###
edit
####
/home/tlab1004/Tlabb/segman/mmlab/configs/deeplabv3plus/deeplabv3plus_r50-d8_4xb2-80k_cityscapes-512x1024.py
#
#train(asedon_HRF)
python tools/train.py /home/tlab1004/Tlabb/segman/mmlab/configs/unet/unet-s5-d16_pspnet_4xb4-40k_hrf-256x256.py



#detect
python demo/image_demo.py demo/demo.png configs/deeplabv3plus/deeplabv3plus_r18b-d8_4xb2-80k_cityscapes-512x1024.py /home/tlab1004/Tlabb/segman/mmlab/work_dirs/deeplabv3plus_r18b-d8_4xb2-80k_cityscapes-512x1024/iter_72000.pth --device cuda:0 --out-file res33ult.jpg

python tools/test.py --config configs/deeplabv3plus/deeplabv3plus_r18b-d8_4xb2-80k_cityscapes-512x1024.py --checkpoint /home/tlab1004/Tlabb/segman/mmlab/work_dirs/deeplabv3plus_r18b-d8_4xb2-80k_cityscapes-512x1024/iter_80000.pth --work-dir /home/tlab1004/Tlabb/segman/mmlab/data/cityscapes/leftImg8bit/test/bielefeld --out /home/tlab1004/Tlabb/segman/mmlab/runs/test --show --show-dir /home/tlab1004/Tlabb/segman/mmlab/runs/test --tta



===================hand 
conda create -n hand python=3.8.8
conda activate hand
pip install mediapipe opencv-python

conda activate hand
cd C:\Users\tiger1005\Documents\Pose
Jupyter Notebook

===================allpose
conda create -n allpose python=3.8.8
conda activate allpose
pip install tensorflow==2.4.1 tensorflow-gpu==2.4.1 opencv-python matplotlib

conda activate allpose
cd C:\Users\tiger1005\Documents\Pose
Jupyter Notebook

===================DETR
conda create -n DETR python=3.8.8
conda activate DETR
conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch

conda activate DETR
cd /home/tlab/Tlabb/ObjectDetection/DETR
path/to/coco/
  annotations/  # annotation json files
  train2017/    # train images
  val2017/      # val images

train
python main.py --coco_path /home/tlab/Tlabb/editdata/yolo2coco/tutorial --epochs 300 --num_workers 16
  
python main.py --coco_path /media/tlab/Data/Dataset/COCO/coco2017 --epochs 1 --num_workers 0 --batch_size 2

Evaluation
python main.py --batch_size 2 --no_aux_loss --eval --resume /home/tlab/Tlabb/ObjectDetection/DETR/runs/train/eval/latest.pth --coco_path /home/tlab/Tlabb/editdata/yolo2coco/tutorial

====================================================Unet
conda acitvate segman
cd /home/tlab4090/Tlabb/segman/unet

python train.py --coco_path /home/tlab/Tlabb/editdata/yolo2coco/tutorial --epochs 300 --num_workers 16



====================================================HR-net
conda create -n HRnet python=3.8
conda activate HRnet
cd /home/tlab4090/Tlabb/segman/HRNet
conda install pytorch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 pytorch-cuda=11.6 -c pytorch -c nvidia
pip install -r requirements.txt

conda activate HRnet
cd /home/tlab4090/Tlabb/segman/HRNet

python -m torch.distributed.launch --nproc_per_node=1 tools/train.py --cfg /home/tlab4090/Tlabb/segman/HRNet/experiments/cityscapes/seg_hrnet_w48_train_512x1024_sgd_lr1e-2_wd5e-4_bs_12_epoch484.yaml

====================================================deepnetv3
conda create -n deepnet python=3.8
conda install pytorch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 pytorch-cuda=11.6 -c pytorch -c nvidia

conda activate deepnet
cd /home/tlab4090/Tlabb/segman/DeepLabV3Plus-Pytorch-master


python main.py --model deeplabv3plus_mobilenet --data_root /home/tlab4090/datasets
/spine

python main.py --model deeplabv3plus_mobilenet --data_root /home/tlab4090/datasets --test_only

=========================================================soup
conda create -n soup python==3.8
conda activate soup
cd /home/tlab4090/Tlabb/modelsoups
conda install pytorch==1.11.0 torchvision==0.12.0 torchaudio==0.11.0 cudatoolkit=11.3 -c pytorch
pip install ftfy regex tqdm
pip install git+https://github.com/openai/CLIP.git@40f5484c1c74edd83cb9cf687c6ab92b28d8b656
pip install wget
pip install git+https://github.com/modestyachts/ImageNetV2_pytorch
pip install requests
pip install matplotlib
pip install pandas

python main.py --download-models --model-location '/home/tlab4090/Tlabb/modelsoups'

python main.py --eval-individual-models --data-location /home/tlab4090/Tlabb/modelsoups' --model-location '/home/tlab4090/Tlabb/modelsoups'

python main.py --uniform-soup --data-location '/home/tlab4090/Tlabb/modelsoups' --model-location '/home/tlab4090/Tlabb/modelsoups'

python main.py --greedy-soup --data-location '/home/tlab4090/Tlabb/modelsoups'--model-location '/home/tlab4090/Tlabb/modelsoups'

python main.py --plot

python main.py --download-models --eval-individual-models --uniform-soup --greedy-soup --plot --data-location <where data is stored> --model-location <where models are stored>


=========================================================
이름바꾸기
1. 확인
rename -n 's/.png/.txt/' *.png
2. 실행
rename 's/.txt/.png/' *.txt

find . -type f -name "*.doc" -exec rename .doc .txt {} \;


3. 데이터 파일 리스트 만들기
find /home/tlab/Tlabb/editdata/yolo2coco/tutorial/val -name '*.jpg' -type f > val.txt
4. 삭제
conda env remove -n HR
=========================================================
LaTex
texmaker

sudo apt-get install xmacro
xmacrorec2 > test.file
for (( i=0; i<100; i++ )); do   xmacroplay "$DISPLAY" <test.file; done



pip install tensorboard
tensorboard --logdir = /home/tlab/logwith each batch collated for minimum padding
cos_lr	False	use cosine learning rate scheduler
close_mosaic	0	(int) disable mosaic augmentation for final epochs
resume	False	resume training from last checkpoint
amp	True	Automatic Mixed Precision (AMP) training, choices=[True, False]
lr0	0.01	initial learning rate (i.e. SGD=1E-2, Adam=1E-3)
lrf	0.01	final learning rate (lr0 * lrf)
momentum	0.937	SGD momentum/Adam beta1
weight_decay	0.0005	optimizer weight decay 5e-4
warmup_epochs	3.0	warmup epochs (fractions ok)
warmup_momentum	0.8	warmup initial momentum
warmup_bias_lr	0.1	warmup initial bias lr
box	7.5	box loss gain
cls	0.5	cls loss gain (scale with pixels)
dfl	1.5	dfl loss gain
pose	12.0	pose loss gain (pose-only)
kobj	2.0	keypoint obj loss gain (pose-only)
label_smoothing	0.0	label smoothing (fraction)
nbs	64	nominal batch size
overlap_mask	True	masks should overlap during training (segment train only)
mask_ratio	4	mask downsample ratio (segment train only)
dropout	0.0	use dropout regularization (classify 
